{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "from  dask import dataframe as dd\n",
    "import shutil\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url = 'https://dadosabertos.ans.gov.br/FTP/PDA/dados_de_beneficiarios_por_operadora/'\n",
    "dir = 'Cancelamento'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extrair SIB Inativos\n",
    "\n",
    "def extrair_links(url):\n",
    "    reponse = requests.get(url)\n",
    "    \n",
    "    if reponse.status_code == 200:\n",
    "        soup = BeautifulSoup(reponse.text, 'html.parser')\n",
    "        \n",
    "        links = soup.find_all('td') \n",
    "\n",
    "        links_ext = []\n",
    "        for link in links:\n",
    "            a_tag = link.find('a')\n",
    "            if a_tag and 'href' in a_tag.attrs:\n",
    "                href = a_tag['href']\n",
    "                if href.startswith('sib_inativo'):\n",
    "                    links_ext.append(href)\n",
    "\n",
    "        return links_ext\n",
    "    else:\n",
    "        print(\"Erro ao acessar a página:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "links = extrair_links(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create urls\n",
    "url_md = [url + i for i in links]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sib_inativo_AC.zip extraído e removido com sucesso.\n",
      "sib_inativo_AL.zip extraído e removido com sucesso.\n",
      "sib_inativo_AM.zip extraído e removido com sucesso.\n",
      "sib_inativo_AP.zip extraído e removido com sucesso.\n",
      "sib_inativo_BA.zip extraído e removido com sucesso.\n",
      "sib_inativo_CE.zip extraído e removido com sucesso.\n",
      "sib_inativo_DF.zip extraído e removido com sucesso.\n",
      "sib_inativo_ES.zip extraído e removido com sucesso.\n",
      "sib_inativo_GO.zip extraído e removido com sucesso.\n",
      "sib_inativo_MA.zip extraído e removido com sucesso.\n",
      "sib_inativo_MG.zip extraído e removido com sucesso.\n",
      "sib_inativo_MS.zip extraído e removido com sucesso.\n",
      "sib_inativo_MT.zip extraído e removido com sucesso.\n",
      "sib_inativo_PA.zip extraído e removido com sucesso.\n",
      "sib_inativo_PB.zip extraído e removido com sucesso.\n",
      "sib_inativo_PE.zip extraído e removido com sucesso.\n",
      "sib_inativo_PI.zip extraído e removido com sucesso.\n",
      "sib_inativo_PR.zip extraído e removido com sucesso.\n",
      "sib_inativo_RJ.zip extraído e removido com sucesso.\n",
      "sib_inativo_RN.zip extraído e removido com sucesso.\n",
      "sib_inativo_RO.zip extraído e removido com sucesso.\n",
      "sib_inativo_RR.zip extraído e removido com sucesso.\n",
      "sib_inativo_RS.zip extraído e removido com sucesso.\n",
      "sib_inativo_SC.zip extraído e removido com sucesso.\n",
      "sib_inativo_SE.zip extraído e removido com sucesso.\n",
      "sib_inativo_SP.zip extraído e removido com sucesso.\n",
      "sib_inativo_TO.zip extraído e removido com sucesso.\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(dir, exist_ok=True)\n",
    "\n",
    "for i, zip_url in enumerate(url_md):\n",
    "    response = requests.get(zip_url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        zip_filename = links[i]  \n",
    "        zip_path = os.path.join(dir, zip_filename)\n",
    "\n",
    "        with open(zip_path, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(dir)\n",
    "\n",
    "        # Remove zip file\n",
    "        os.remove(zip_path)\n",
    "        print(f\"{zip_filename} extraído e removido com sucesso.\")\n",
    "    else:\n",
    "        print('Não foi possível baixar o arquivo .zip. Status code:', response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "    'DT_NASCIMENTO': object,\n",
    "    'DT_REATIVACAO': object,\n",
    "    'DT_ULTIMA_REATIVACAO': object,\n",
    "    'DT_ULTIMA_MUDA_CONTRATUAL':object,\n",
    "    'NR_PLANO_SCPA': object,\n",
    "    'LG_RESIDE_EXTERIOR': object,\n",
    "    'ID_MOTIVO_MOVM_CANCELAMENTO':object,\n",
    "    'LG_COBERTURA_PARCIAL_TEMP': object,\n",
    "    'ID_TIPO_DEPENDENTE': object,\n",
    "    'LG_ITEM_EXCLUIDO_COBERTURA': object,\n",
    "    'NR_PLANO_RPS': object,\n",
    "    'ID_MOTIVO_MOVM_INCLUSAO':object,\n",
    "    'ID_MOTIVO_MOVIMENTO': object,\n",
    "    'CD_PLANO_SCPA':object,\n",
    "    'ID_BENE_TIPO_DEPENDENTE':object,\n",
    "    'LG_ITENS_EXCLUIDO_COBERTURA':object,\n",
    "    'ID_BENE_MOTIV_CANCELAMENTO':object,\n",
    "    'CD_PLANO_RPS': object,\n",
    "    'CD_BENE_MOTV_INCLUSAO': object\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_filtrados = []\n",
    "for arquivo in os.listdir(dir):\n",
    "    if arquivo.endswith('.csv'):\n",
    "        caminho_arquivo = os.path.join(dir, arquivo)\n",
    "\n",
    "        # Lê com Dask\n",
    "        try:\n",
    "            df = dd.read_csv(caminho_arquivo, sep=\";\", encoding='ISO-8859-1', dtype= dtypes)\n",
    "\n",
    "            # Filtra os anos 2024 e 2025\n",
    "            df_filtrado = df[df['DT_ULTIMO_CANCELAMENTO'].str.contains('^2025-|^2024-', regex=True, na=False)]\n",
    "\n",
    "            dfs_filtrados.append(df_filtrado)\n",
    "\n",
    "            # Exclui o arquivo após ler e filtrar\n",
    "            #os.remove(caminho_arquivo)\n",
    "            #print(f'{arquivo} processado e excluído.')\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f'Erro ao processar {arquivo}: {e}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo consolidado salvo com sucesso.\n"
     ]
    }
   ],
   "source": [
    "# Consolida tudo em um DataFrame Dask\n",
    "if dfs_filtrados:\n",
    "    df_consolidado = dd.concat(dfs_filtrados)\n",
    "    op = [5711,327417,335100,350249,366561,368253]\n",
    "    df_consolidado = df_consolidado[df_consolidado['REGISTRO_OPERADORA'].isin(op)]\n",
    "\n",
    "    # Opcional: converter para pandas se for pequeno o suficiente\n",
    "    df_resultado = df_consolidado.compute()\n",
    "    \n",
    "    # Salva em um novo arquivo CSV consolidado\n",
    "    df_resultado.to_csv('consolidado_2024_2025.csv', index=False)\n",
    "    print(\"Arquivo consolidado salvo com sucesso.\")\n",
    "else:\n",
    "    print(\"Nenhum dado correspondente encontrado.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
